# üöÄ Advanced RAG Pipeline

A modular, production-ready Retrieval-Augmented Generation (RAG) system designed for nuclear regulatory documents (NQA-1, ASME, IAEA standards). Features multiple retrieval strategies, cross-encoder reranking, and agentic reasoning.

---

## üìã Table of Contents

- [Quick Start](#-quick-start)
- [User Guide](#-users-guide)
- [Query Workflows](#-query-workflows)
- [Configuration Guide](#-configuration-guide)
- [Architecture](#-architecture)
- [Troubleshooting](#-troubleshooting)
- [Advanced Features](#-advanced-features)

---

## üöÄ Quick Start

> **Prerequisites**: Python 3.10+, Ollama installed and running

### 1. Initial Setup (One-time)

\`\`\`bash
# Activate your virtual environment
cd ~/my-first-rag/rag-advanced
source .venv/bin/activate  # or: .venv\Scripts\activate on Windows

# Verify Ollama is running
curl http://localhost:11434/api/tags
\`\`\`

### 2. Add Your Documents

\`\`\`bash
# Place your PDF documents in the data/ folder
cp /path/to/your/*.pdf data/
\`\`\`

### 3. Ingest Documents

**Option A: All-in-one (Recommended)**
\`\`\`bash
# Create vector store + BM25 index in one command
python -m scripts.ingest --strategy section --build-bm25
\`\`\`

**Option B: Step-by-step**
\`\`\`bash
# Step 1: Create vector store with section-aware chunking
python -m scripts.ingest --strategy section

# Step 2: Build BM25 index for hybrid search
# ‚ö†Ô∏è IMPORTANT: Must specify same strategy to avoid duplicate chunks
python -m scripts.ingest --build-bm25 --strategy section
\`\`\`

> **üí° Why section strategy?** For regulatory documents (NQA-1, ASME), section-aware chunking respects document structure (Section, Article, Clause, Annex) for more coherent retrieval.

> **‚ö†Ô∏è Known Issue**: If you run `--build-bm25` without specifying `--strategy`, it will default to `recursive` and re-chunk documents differently, creating duplicates. Always specify the strategy explicitly or use Option A.

### 4. Start Querying

\`\`\`bash
# Basic mode (vector search only)
python -m scripts.query

# Or try hybrid mode (vector + keyword search)
python -m scripts.query --mode hybrid
\`\`\`

---

## üìñ User Guide

### Understanding the Pipeline

Query flows through retrieval ‚Üí (optional reranking) ‚Üí LLM generation

**Basic Flow:**
```
PDF ‚Üí Embed ‚Üí ChromaDB ‚Üí Vector Search ‚Üí Top-K ‚Üí LLM ‚Üí Answer
```

**With Reranking:**
```
PDF ‚Üí Embed ‚Üí ChromaDB ‚Üí Vector Search ‚Üí Top-8 ‚Üí Reranker ‚Üí Top-4 ‚Üí LLM ‚Üí Answer
```

### Example Queries

**Good for this system:**
‚úÖ "What are QA Level 1 requirements?"
‚úÖ "Explain Section 18.1 of NQA-1"
‚úÖ "What does 10 CFR 50 Appendix B require?"
‚úÖ "Compare ASME and NQA-1 inspection requirements"

**Not ideal:**
‚ùå "What is quality assurance?" (too broad, not in documents)
‚ùå "Who wrote NQA-1?" (metadata not in content)

---

## üîÑ Query Workflows

### Mode 1: Basic (Vector Search)

**Best for:** Conceptual questions, general understanding

```
You: What are quality assurance principles?
  ‚Üì
[Vector Search] ‚Üí Finds semantically similar chunks
  ‚Üì
[LLM] ‚Üí Generates answer from retrieved context
```

**Command:**
```bash
python -m scripts.query
```

**Pipeline:**
```mermaid
graph LR
    A[Query] --> B[Embed Query]
    B --> C[Vector Search]
    C --> D[Top 4 Chunks]
    D --> E[LLM]
    E --> F[Answer]

    style A fill:#e3f2fd
    style F fill:#e8f5e9
```

---

### Mode 2: Hybrid (Vector + BM25)

**Best for:** Specific sections, technical terms, exact phrases

```
You: What does NQA-1 Section 18 require?
  ‚Üì
[Vector Search] ‚Üí Semantic similarity
[BM25 Search] ‚Üí Exact keyword matching  
  ‚Üì
[RRF Fusion] ‚Üí Combines both results
  ‚Üì
[LLM] ‚Üí Generates answer
```

**Command:**
```bash
python -m scripts.query --mode hybrid
```

**Pipeline:**
```mermaid
graph LR
    A[Query] --> B[Vector Search]
    A --> C[BM25 Search]
    B --> D[RRF Fusion]
    C --> D
    D --> E[Top 4 Chunks]
    E --> F[LLM]
    F --> G[Answer]

    style A fill:#e3f2fd
    style D fill:#fff9c4
    style G fill:#e8f5e9
```

**Requirements:**
```bash
# Must build BM25 index first
python -m scripts.ingest --build-bm25
```

---

### Mode 3: Hybrid + Reranking + MMR (Recommended)

**Best for:** Maximum precision, technical documents, cross-jurisdictional comparison

```
You: Explain QA Level 1 requirements
  ‚Üì
[Vector + BM25] ‚Üí Fetch top 50 candidates
  ‚Üì
[Cross-Encoder Reranker] ‚Üí Scores each query-doc pair
  ‚Üì
[MMR Diversity Selection] ‚Üí Selects 4 relevant AND diverse chunks
  ‚Üì
[LLM] ‚Üí High-quality answer with diverse perspectives
```

**Enable reranking + MMR (enabled by default):**
```python
# Edit config/settings.py
RERANKER_ENABLED = True  # Already enabled by default
MMR_LAMBDA_MULT = 0.7    # Adjust for more/less diversity
```

**Command:**
```bash
python -m scripts.query --mode hybrid
# Will show: "üîÑ Hybrid retrieval with reranking enabled"
```

**Pipeline:**
```mermaid
graph LR
    A[Query] --> B[Vector Search K=50]
    A --> C[BM25 Search K=50]
    B --> D[RRF Fusion]
    C --> D
    D --> E[Top 50 Chunks]
    E --> F[Reranker]
    F --> G[Top 50 Scored]
    G --> H[MMR Diversity]
    H --> I[Top 4 Diverse]
    I --> J[LLM]
    J --> K[Answer]

    style A fill:#e3f2fd
    style F fill:#ffecb3
    style H fill:#e1bee7
    style K fill:#e8f5e9
```

**Benefits:**
- üéØ Better precision for technical queries
- üìä 15-30% improvement in relevance
- üöÄ GPU accelerated (50-150ms latency)

---

### Mode 4: Agentic (Multi-Step Reasoning)

**Best for:** Complex questions requiring multiple lookups

```
You: Compare NQA-1 and ASME QA requirements
  ‚Üì
[Agent Decides] ‚Üí "I need to search both standards"
  ‚Üì
[Tool 1: Search NQA-1] ‚Üí Retrieves NQA-1 info
[Tool 2: Search ASME] ‚Üí Retrieves ASME info
  ‚Üì
[Agent Synthesizes] ‚Üí Compares and contrasts
```

**Command:**
```bash
python -m scripts.query --mode agentic
```

**Features:**
- üß† LLM decides which tools to use
- üîç Multiple retrieval rounds
- üìù Shows reasoning steps
- ‚ö° Up to 5 steps before final answer

---

## ‚öôÔ∏è Configuration Guide

All settings are in `config/settings.py`. Edit this file to customize behavior.

### Core Models

```python
# Chat model (for generating answers)
CHAT_MODEL = "gemma3:4b"  # Options: gemma3:4b, qwen2.5:32b, llama3.1:70b

# Embedding model (for vector search)
EMBED_MODEL = "mxbai-embed-large"  # 1024-dim, high performance

# Ollama server
OLLAMA_BASE_URL = "http://localhost:11434"
```

**üí° Tip:** Start with small models for testing, then scale up:
- Testing: `gemma3:4b` (~2.5 GB VRAM)
- Production: `qwen2.5:32b` or `nemotron`

---

### Chunking Strategy

```python
# Standard chunking (default)
CHUNK_SIZE = 1000          # Characters per chunk
CHUNK_OVERLAP = 200        # Overlap between chunks

# Section-aware chunking (for standards)
SECTION_CHUNK_SIZE = 500
SECTION_CHUNK_OVERLAP = 100
```

**When to use section-aware:**
```bash
# Use for documents with clear section structure
python -m scripts.ingest --strategy section
```

---

### Retrieval Settings

```python
# Number of chunks to retrieve
RETRIEVER_K = 4

# Hybrid search weights
HYBRID_VECTOR_WEIGHT = 0.6   # 60% semantic
HYBRID_BM25_WEIGHT = 0.4     # 40% keyword
```

**Tuning tips:**
- ‚¨ÜÔ∏è Increase `RETRIEVER_K` (e.g., 6-8) for complex questions
- ‚¨áÔ∏è Decrease for focused, specific queries
- Adjust weights based on your document type

---

### Reranking Settings

```python
# Enable/disable
RERANKER_ENABLED = True    # Default: enabled for better results

# Model selection
RERANKER_MODEL = "Qwen/Qwen3-Reranker-4B"  # Multilingual, high accuracy

# Performance
RERANKER_USE_FP16 = True     # Faster on GPU
RERANKER_DEVICE = "cuda"      # or "cpu"
RERANKER_BATCH_SIZE = 32

# Retrieval behavior
RERANKER_FETCH_K = 50   # Fetch 50, rerank all, MMR selects top 4 diverse
```

**MMR Diversity Selection:**
```python
# When reranking is enabled, MMR is automatically applied
MMR_LAMBDA_MULT = 0.7        # Balance relevance (0.7) vs diversity (0.3)
                              # 1.0 = max relevance, 0.0 = max diversity
MMR_FETCH_K = 50              # Candidates to fetch before MMR selection
```

**Custom fine-tuned model:**
```python
# Train on your domain, then use:
RERANKER_CUSTOM_MODEL_PATH = "/path/to/fine-tuned-model"
```

---

### Agentic Settings

```python
MAX_AGENT_STEPS = 5          # Max reasoning iterations
AGENT_TEMPERATURE = 0.1      # Low = factual, High = creative
```

---

## üèóÔ∏è Architecture

### Project Structure

```
rag-advanced/
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ settings.py          # üéõÔ∏è All configuration here
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ embeddings.py         # Embedding model wrapper
‚îÇ   ‚îú‚îÄ‚îÄ store.py              # ChromaDB vector store
‚îÇ   ‚îî‚îÄ‚îÄ llm.py                # LLM wrapper (Ollama)
‚îú‚îÄ‚îÄ modules/
‚îÇ   ‚îú‚îÄ‚îÄ chunking.py           # Text splitting strategies
‚îÇ   ‚îú‚îÄ‚îÄ hybrid_search.py      # BM25 + Vector fusion
‚îÇ   ‚îú‚îÄ‚îÄ reranking.py          # üÜï Cross-encoder reranker
‚îÇ   ‚îú‚îÄ‚îÄ multilingual.py       # Prompt templates
‚îÇ   ‚îú‚îÄ‚îÄ agentic.py            # Multi-step reasoning
‚îÇ   ‚îî‚îÄ‚îÄ knowledge_graph.py    # Entity extraction
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ ingest.py             # PDF ‚Üí Vector store
‚îÇ   ‚îú‚îÄ‚îÄ query.py              # Interactive querying
‚îÇ   ‚îî‚îÄ‚îÄ reset_store.py        # Clear database
‚îú‚îÄ‚îÄ data/                     # üìÑ Your PDFs here
‚îú‚îÄ‚îÄ chroma_db/                # üóÑÔ∏è Persistent vector store
‚îú‚îÄ‚îÄ bm25_index.pkl            # Keyword search index
‚îî‚îÄ‚îÄ requirements.txt
```

### Module Overview

| Module | Purpose | Priority |
|--------|---------|----------|
| Persistent Store | Skip re-embedding on reruns | ‚úÖ P1 |
| Chunking | Section-aware splitting | ‚úÖ P2 |
| Hybrid Search | Vector + BM25 fusion | ‚úÖ P3 |
| Prompts | Optimized templates | ‚úÖ P4 |
| Agentic RAG | Multi-step reasoning | ‚úÖ P5 |
| Knowledge Graph | Relationship extraction | ‚úÖ P6 |
| **Reranking** | **Cross-encoder precision** | **‚úÖ P7** |

---

## üîß Troubleshooting

### Issue: Dimension Mismatch Error

```
chromadb.errors.InvalidArgumentError: Collection expecting embedding with dimension of 768, got 1024
```

**Cause:** Changed embedding model after creating vector store

**Solution:**
```bash
# Reset and rebuild with new embeddings
python -m scripts.reset_store
python -m scripts.ingest --strategy section --build-bm25
```

---

### Issue: No BM25 Index Found

```
‚ùå No BM25 index found. Run: python -m scripts.ingest --build-bm25
```

**Solution:**
```bash
# Build BM25 index from existing chunks
# ‚ö†Ô∏è Must match the chunking strategy used during initial ingestion
python -m scripts.ingest --build-bm25 --strategy section

# Or if you used default recursive strategy:
python -m scripts.ingest --build-bm25 --strategy recursive
```

---

### Issue: Duplicate Chunks in Vector Store

**Symptom:** After running `--build-bm25`, your chunk count increased unexpectedly:
```
First run:  446 chunks (strategy: section)
Second run: 679 chunks (added 233 more with strategy: recursive)
```

**Cause:** Ran `--build-bm25` without specifying `--strategy`, causing documents to be re-chunked with default `recursive` strategy and added as duplicates.

**How to check:**
```bash
# Check your vector store size
python -c "from core.store import VectorStoreManager; m = VectorStoreManager(); print(f'Total chunks: {len(m.get_store().get()[\"ids\"])}')"
```

**Solution: Reset and re-ingest**
```bash
# 1. Clear duplicate chunks
python -m scripts.reset_store

# 2. Re-ingest with single strategy (all-in-one approach)
python -m scripts.ingest --strategy section --build-bm25

# Now you have clean chunks ready for vector/hybrid/agentic modes ‚úÖ
```

---

### Issue: Reranker GPU Out of Memory

```
CUDA out of memory
```

**Solution 1: Use CPU**
```python
# config/settings.py
RERANKER_DEVICE = "cpu"
```

**Solution 2: Disable FP16**
```python
RERANKER_USE_FP16 = False
```

**Solution 3: Reduce batch size**
```python
RERANKER_BATCH_SIZE = 8  # Down from 32
```

---

### Issue: Slow Query Performance

**Diagnose:**
- Vector search: ~50-200ms ‚úÖ
- BM25 search: ~20-50ms ‚úÖ
- Reranking (GPU): ~50-150ms ‚úÖ
- Reranking (CPU): ~200-500ms ‚ö†Ô∏è
- LLM generation: 2-10s (depends on model) ‚ö†Ô∏è

**Optimization tips:**
1. Use smaller chat model for testing
2. Enable GPU for reranking
3. Reduce `RETRIEVER_K` if not needed
4. Disable reranking for simple queries

---

### Issue: Poor Answer Quality

**Try these in order:**

1. **Enable hybrid search:**
```bash
# Build BM25 index (match your chunking strategy)
python -m scripts.ingest --build-bm25 --strategy section
python -m scripts.query --mode hybrid
```

2. **Enable reranking:**
```python
# config/settings.py
RERANKER_ENABLED = True
```

3. **Increase retrieval:**
```python
RETRIEVER_K = 6  # Up from 4
```

4. **Try agentic mode:**
```bash
python -m scripts.query --mode agentic
```

5. **Use larger chat model:**
```python
CHAT_MODEL = "qwen2.5:32b"  # Up from gemma3:4b
```

---

### Issue: Results Too Similar / Not Diverse Enough

**Symptom:** Retrieved documents are all from the same section or very similar content

**Solution: Adjust MMR diversity parameter**
```python
# config/settings.py
MMR_LAMBDA_MULT = 0.5  # Down from 0.7 for MORE diversity
```

**Other options:**
- Increase `MMR_FETCH_K` to 100 for more candidate diversity
- Check if your query is too specific (narrow queries naturally have less diversity)

---

## üöÄ Advanced Features

### Fine-Tuning the Reranker

For domain-specific performance (nuclear terminology), fine-tune the reranker:

```python
from modules.reranking import create_training_data, export_training_data

# 1. Collect query-document pairs
queries = ["What are QA Level 1 requirements?", ...]
relevant_docs = [[doc1, doc2], ...]     # Relevant for each query
irrelevant_docs = [[doc3, doc4], ...]   # Irrelevant for each query

# 2. Create training data
training_data = create_training_data(queries, relevant_docs, irrelevant_docs)

# 3. Export for fine-tuning
export_training_data(training_data, "nuclear_reranker_train.jsonl")

# 4. Follow BGE fine-tuning guide
# https://github.com/FlagOpen/FlagEmbedding

# 5. Use fine-tuned model
# config/settings.py
RERANKER_CUSTOM_MODEL_PATH = "/path/to/fine-tuned-model"
```

---

### Knowledge Graph Queries

Extract and query entity relationships:

```bash
# Build knowledge graph
python -m scripts.ingest --build-kg

# Query entities
python -m scripts.query --mode kg
```

**Example:**
```
Entity: NQA-1
  ‚Üí Outgoing: requires ‚Üí Document Control
  ‚Üí Outgoing: references ‚Üí 10 CFR 50
  ‚Üí Incoming: implements ‚Üê ASME
```

---

### Batch Processing

Process multiple queries programmatically:

```python
from core.store import VectorStoreManager
from core.llm import get_llm
from modules.reranking import get_reranker, RerankingRetriever

manager = VectorStoreManager()
reranker = get_reranker()  # if RERANKER_ENABLED = True

retriever = manager.get_retriever(k=8)
if reranker:
    from modules.reranking import RerankingRetriever
    retriever = RerankingRetriever(retriever, reranker, top_k=4)

queries = ["Query 1", "Query 2", ...]
for query in queries:
    docs = retriever.invoke(query)
    # Process docs...
```

---

## üìö Additional Resources

- **FlagEmbedding (Reranker):** https://github.com/FlagOpen/FlagEmbedding
- **LangChain Docs:** https://python.langchain.com/
- **Ollama Models:** https://ollama.com/library
- **ChromaDB:** https://docs.trychroma.com/

---

## üéØ Quick Reference

| Task | Command |
|------|---------|
| Initial setup (vector + BM25) | `python -m scripts.ingest --strategy section --build-bm25` |
| Initial setup (vector only) | `python -m scripts.ingest --strategy section` |
| Add BM25 to existing store | `python -m scripts.ingest --build-bm25 --strategy section` ‚ö†Ô∏è |
| Basic query | `python -m scripts.query` |
| Hybrid query (needs BM25) | `python -m scripts.query --mode hybrid` |
| Agentic query | `python -m scripts.query --mode agentic` |
| KG query | `python -m scripts.query --mode kg` |
| Reset database | `python -m scripts.reset_store` |
| Change embedding model | Edit `config/settings.py` ‚Üí `EMBED_MODEL` ‚Üí Reset & re-ingest |
| Enable reranking | Edit `config/settings.py` ‚Üí `RERANKER_ENABLED = True` |

> ‚ö†Ô∏è When adding BM25 to an existing store, **always specify the same chunking strategy** used during initial ingestion to avoid duplicate chunks.

---

Made with ‚ù§Ô∏è for nuclear regulatory document retrieval
